{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "S1wbTW43zW4b",
   "metadata": {
    "id": "S1wbTW43zW4b"
   },
   "source": [
    "## **Development of Document Processing System Using OCR, LLM, Text Summarization Techniques and Image Generation**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "To design and implement a system that extracts text from image-based PDFs, generates a conversational chatbot based on the extracted text, summarizes the extracted text\n",
    "\n",
    "### **User Instructions:**\n",
    "1.\tThe python file is in Notebook format (.ipynb). The file can be run using `Google Colab`.\n",
    "2.\tGo to Colab, click `upload notebook` and upload the notebook file.\n",
    "3.\tNext step is to upload the PDF file that needed to generated from. Upload the document in /content/ folder. Change the PDF name while instantiating the `PyPDFLoader ` class.\n",
    "4. Create `images` folder inside `\\content\\` folder, for the images to save.\n",
    "4.\tChange the `query` variable accordingly.\n",
    "5.\tRun the cells by clicking `runtime -> run all`. This will all the cells and gives the output of each cell.\n",
    "\n",
    "### **Implementation Details**:\n",
    "\n",
    "1.\t**Import necessary libraries:** The first step of our implementation is to install and import all the necessary libraries that we need. The libraries that are used are: `opencv`, `easyocr`, `PyPDF2`, `Groq`.\n",
    "2.\t**Read and extract images from the PDF:** Read the given PDF file by using `PdfReader` class from `PyPDF2` library and save all the images from each page into seperate image file inside images folder.\n",
    "3.\t**Extract text from the images:** Iterate through all the image files and extract the text and save it as a variable.\n",
    "5.\t**Generating Answer:** The retrieved content along with the user query is used as input to generate an answer using the `llama2-70b-4096` model which we can use from `Groq` API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629ee470",
   "metadata": {
    "id": "629ee470"
   },
   "source": [
    "### **Install and Import Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac0b451",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cac0b451",
    "outputId": "67105f3b-b121-446f-ef0c-86e40142a336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n"
     ]
    }
   ],
   "source": [
    "# Installing the required modules\n",
    "!pip install opencv-python\n",
    "!pip install matplotlib\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4227cda2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4227cda2",
    "outputId": "1532ec85-b5a1-4d6f-8a2e-514e0ef00697"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: easyocr in /usr/local/lib/python3.10/dist-packages (1.7.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.1.0+cu121)\n",
      "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.16.0+cu121)\n",
      "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from easyocr) (4.9.0.80)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.11.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.25.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from easyocr) (9.4.0)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.19.3)\n",
      "Requirement already satisfied: python-bidi in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.4.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from easyocr) (6.0.1)\n",
      "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.3)\n",
      "Requirement already satisfied: pyclipper in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.3.0.post5)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.11.1.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5->easyocr) (2.31.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from python-bidi->easyocr) (1.16.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2.31.6)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2024.2.12)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (1.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (23.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->easyocr) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5->easyocr) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5->easyocr) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5->easyocr) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5->easyocr) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->easyocr) (1.3.0)\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "# Installing the CPU and CUDA\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install easyocr\n",
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EowdxuCh0WBh",
   "metadata": {
    "id": "EowdxuCh0WBh"
   },
   "source": [
    "### **Importing all the neccessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d82886ac",
   "metadata": {
    "id": "d82886ac"
   },
   "outputs": [],
   "source": [
    "# Importing the different libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import PyPDF2 # For processing PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SJywy0pz0k64",
   "metadata": {
    "id": "SJywy0pz0k64"
   },
   "source": [
    "### **Extracting all the images from the PDF and save it in seperate folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZINDoS-8Y6-E",
   "metadata": {
    "id": "ZINDoS-8Y6-E"
   },
   "outputs": [],
   "source": [
    "def extract_image_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        # Create a PDF reader object\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "        image_count = 1\n",
    "\n",
    "        # Loop through each page in the PDF\n",
    "        for page in pdf_reader.pages:\n",
    "            # getting the image from the page. Since each page has only one image,\n",
    "            # we are getting the 1st image\n",
    "            image = page.images[0]\n",
    "\n",
    "            # Creating image file for the extracted image\n",
    "            with open(f'./images/image{image_count}.jpg', \"wb\") as fp:\n",
    "              fp.write(image.data)\n",
    "              image_count += 1\n",
    "\n",
    "# calling the function, which will create image file for each images in the pdf.\n",
    "extract_image_from_pdf('session5.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531064c7",
   "metadata": {
    "id": "531064c7"
   },
   "source": [
    "### **Defining the path of images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "05b9815c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05b9815c",
    "outputId": "3f50af91-e930-4064-9e6e-e25657746bed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image1.jpg\n",
      "image2.jpg\n",
      "image3.jpg\n",
      "image4.jpg\n",
      "image5.jpg\n"
     ]
    }
   ],
   "source": [
    "# Extracting the path of images one by one in the form of list\n",
    "img_path = \"./images/\"\n",
    "create_path = lambda f : os.path.join(img_path, f)\n",
    "\n",
    "# listing all the images in the directory as a list\n",
    "test_image_files = [f for f in os.listdir(img_path) if '.jpg' in f.lower()]\n",
    "test_image_files.sort()\n",
    "\n",
    "for f in test_image_files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u6hYdUFs2DZK",
   "metadata": {
    "id": "u6hYdUFs2DZK"
   },
   "source": [
    "### **Defining a function that regocnize text from a single image file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2dd58ad0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2dd58ad0",
    "outputId": "259495a0-753c-4291-9fad-68cf87f19afe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "# Defining reader object for extract text in English ('en')\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# Function that retruns the recognized text as list with prob, text for each line\n",
    "def recognize_text(img_path):\n",
    "    ''' loads an image and recognizes text. '''\n",
    "    return reader.readtext(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8QzpUzB25uD",
   "metadata": {
    "id": "b8QzpUzB25uD"
   },
   "source": [
    "### **Defining function to extract text from image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "JUQLG6kyt7iC",
   "metadata": {
    "id": "JUQLG6kyt7iC"
   },
   "outputs": [],
   "source": [
    "# Extracting text from the image\n",
    "def ocr_text(img_path):\n",
    "    # recognizing text from an image\n",
    "    result = recognize_text(img_path)\n",
    "    res = \"\"  # result variable to concate all the lines of an single image file\n",
    "\n",
    "    # Iterating over all the lines along with probability of recognized text\n",
    "    for(bbox, text, prob) in result:\n",
    "        # If OCR prob is over 0.2, overlay text:\n",
    "        if prob:\n",
    "            res += text\n",
    "\n",
    "    # returning extracted text of the image file\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Frs_RaCy4mlB",
   "metadata": {
    "id": "Frs_RaCy4mlB"
   },
   "source": [
    "### **Extract text from all the images in the PDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cf2835",
   "metadata": {
    "id": "44cf2835"
   },
   "outputs": [],
   "source": [
    "# loading the path of image by passing the position of the image\n",
    "extracted_text = \"\"\n",
    "\n",
    "# Iterating over list of image files present in PDF:\n",
    "for image_file in test_image_files:\n",
    "  # creating complete path of the current image\n",
    "  path = create_path(image_file)\n",
    "\n",
    "  # Extracting the text from that image and concatinating it with the result\n",
    "  extracted_text += ocr_text(path)\n",
    "\n",
    "# Now the `extracted_text varibale` contains text from all the images in the PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xvloevvg5hGa",
   "metadata": {
    "id": "Xvloevvg5hGa"
   },
   "source": [
    "### **Printing the extracted text from the image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0469efac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "0469efac",
    "outputId": "e56515ee-5d56-4546-a8e7-2dd5690da870",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'ArtificialintelligencesystemLearns fronUses theRecognisesSolvesUnderstandsCreatesexperiencelearningmagescomplexlanguageperspectivest0 reasonproblemsits nuancesArtificial intelligence (AI), sometimes called machine intelligence,is intelligence demonstrated by machines_in contrast to the naturalintelligence displayed by humans and other animals, such as \"learningand \"problem solving_In computer science AI research is defined as the study of\"intelligent agentsany device that perceives its environment andtakes actions that maximize its chance of successfully achieving itsgoals:auldWays that People Think and LearnAboutThingsIf you havea problem, think of a past situationwhere you solveda similar problem_If you take an action, anticipate what might happennext.Iffail at something, imagine how you mighthave done things differently:If you observe an event,to infer what prior eventmightcaused it_If you see an object, wonder if anyone owns it:If someone does something, ask yourself what theperson\\'s purpose was in doing that_youtryhaveKEY RESEARCH AREAS IN AIProblem solving, planning, and searchgeneric problem solvingarchitecture basedonideas fromcognitive science (game playing,robotics).Knowledge Representationto store and manipulate information(logical and probabilistic representations)Automated reasoningInferenceto use the stored information toanswerquestions and draw new conclusionsMachine Learningintelligence from data; to adapt to newcircumstances and to detect and extrapolate patternsNatural Language Processingto communicate with the machineComputer Visionprocessing visual informationRoboticsAutonomy, manipulation, full integration of AIcapabilitiesFrom SIRI and Alexa, to self-driving cars, artificialintelligence (AI) is progressing rapidly:While science fiction often portrays AI as robots with human-likecharacteristics, AI can encompass anything from Google\\'s searchalgorithms, to IBM\\'s Watson, to autonomous weapons_Artificial intelligence today is properly known as narrow AILor_weak ALL in that it is designed to performa narrowtask such as only facial recognition, or only internetsearches, or only driving a car).However, the long-term goal of many researchers is tocreate general AL (AGLor strong AI)While narrow AI may outperform humans at whatever itsspecific task is, like playing chess or solving equations, AGIwould outperform humans at nearly every thinking taskGoogle announced their Duplex system, a new technology forconducting natural conversations to carry out\\'real world\"tasks overthe phone.The technology is directed towards completing specific tasks, suchasscheduling certain types of appointmentsFor such tasks, the system makes the conversational experience asnatural as possible, allowing people to speak normally, like theywould to another person, without having to adapt to a machine_DUPLEXTALKING Al,\"Hi,Im calling to book awomen\\'s haircut for a client:\\'https:[Lwwwyoutube comlwatchZv-GoXplleASQc'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8vLTf_Io5tYT",
   "metadata": {
    "id": "8vLTf_Io5tYT"
   },
   "source": [
    "## **Developing chatbot based on the interpreted text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e0RfsNAKxcPs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0RfsNAKxcPs",
    "outputId": "d5539d22-3561-4156-e7dc-796b1df85d07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.6.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.6)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.16.3)\n"
     ]
    }
   ],
   "source": [
    "''' Installing groq for using llama model with API '''\n",
    "!pip install groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qnB7F4lD6Lsv",
   "metadata": {
    "id": "qnB7F4lD6Lsv"
   },
   "source": [
    "### **Summerizing the text in the images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZbLniq2oxA_D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZbLniq2oxA_D",
    "outputId": "c180975d-2d01-4714-88fa-8e82f110ce16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) is a field of study in computer science that focuses on creating machines that can perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making. AI systems can be used for a variety of tasks, including facial recognition, internet searches, and driving cars. The long-term goal of many researchers is to create general AI (AGI) that can outperform humans at nearly every thinking task. One recent development in AI is Google's Duplex system, which can conduct natural conversations over the phone to carry out real-world tasks, such as scheduling appointments."
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "# setting environment variable for groq\n",
    "os.environ['GROQ_API_KEY'] = 'Your Groq API Key Here'\n",
    "\n",
    "# creating groq client to use the model `llama2-70b-4096`\n",
    "client = Groq()\n",
    "\n",
    "\n",
    "# giving prompt for summerizing the content\n",
    "query = \"Give me summery of the context\"\n",
    "prompt = f'''\n",
    "    You are a chatbot you must generate a good summarised answer from the context.\n",
    "    Use the following provided context to answer the query enclosed within triple backticks.\n",
    "    Context: {extracted_text}\n",
    "    User Query: ```{query}```\n",
    "    Answer:\n",
    "'''\n",
    "\n",
    "# generating answer from the context and query using 'llama2-70b-4096' model:\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"llama2-70b-4096\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\" {prompt}\n",
    "            \"\"\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.5,\n",
    "    max_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=True,\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "# printing the result\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "KsEpNvb0ySFx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KsEpNvb0ySFx",
    "outputId": "ca1031ea-adb8-4512-a211-ba8125c2b1e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several ways that people think and learn about things. One way is by using past experiences to solve similar problems. People can also use anticipation and imagination to think about what might happen next or how they could have done things differently. Observation is another way people learn, by inferring what might have caused an event or who might own an object. Additionally, people can use automated reasoning to answer questions and draw new conclusions, and machine learning to adapt to new circumstances and detect patterns. Finally, natural language processing and computer vision allow people to communicate with machines and process visual information. These ways of thinking and learning are being used to develop artificial intelligence (AI) systems that can perform tasks such as facial recognition, internet searches, and driving a car. The long-term goal of many researchers is to create general AI that can outperform humans at nearly every thinking task."
     ]
    }
   ],
   "source": [
    "# giving prompt for query\n",
    "query = \"What are the ways that people think and learn about things?\"\n",
    "prompt = f'''\n",
    "    You are a chatbot you must generate a good summarised answer from the context.\n",
    "    Use the following provided context to answer the query enclosed within triple backticks.\n",
    "    Context: {extracted_text}\n",
    "    User Query: ```{query}```\n",
    "    Answer:\n",
    "'''\n",
    "\n",
    "# generating answer from the context and query using 'llama2-70b-4096' model:\n",
    "completion1 = client.chat.completions.create(\n",
    "    model=\"llama2-70b-4096\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\" {prompt}\n",
    "            \"\"\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.5,\n",
    "    max_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=True,\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "# printing the result\n",
    "for chunk in completion1:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ocr-venv",
   "language": "python",
   "name": "ocr-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
